### `report-1:` Training a simple neural network model with different optimizers, loss function, and learning rates on the Iris dataset + EDA

In this notebook, I did EDA on the Iris dataset. Also, I defined a simple neural network, and I trained this network on the aforementioned dataset with different settings. I changed the optimizer, loss function, and learning rate to measure their importance on this dataset.

### `report-2:` Exploratory Data Analysis - SQuADv2.0

In this report, I implemented notebook of Exploratory Data Analysis (EDA) on the SQuAD dataset, a question answering dataset, and also used predictions of my groupmate's model for this task to evaluate them with the baseline and human answers.

### `report-3:` Explorary Data Analysis - Flicker8k, MSCOCO-Captions, and VizWiz-Captions

I compared between three Image Captioning datasets, Flickr8k, MSCOCO-Captions, and VizWiz-Captions, in different terms.
Also, I evaluated Models implemented by my groupmates on Flickr8k and compared the results together.

### `report-4:` Implementing Different Data Augmentation Methods and Explorary Data Analysis on ImageNet

In this notebook, I did a simple EDA on ImageNet. In the next part of this report, I implement several data augmentation methods such as MixUp, CutMix, PCA, Image translation, and DeepAugmentation. Finaly, I evaluate three models: Baseline model which is ResNet-50, ResNet-50 + MixUp, and ResNet-50 + CutMix.


### `report-5:` End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF

In this notebook, first of all, we did some analysis on CoNLL 2003 dataset. Then, we trained the model on two different word embeddings to observe their importance. When we changed word embeddings from Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 100d) to Twitter (27B tokens, 1.2M vocab, uncased, 100d) and train the model on both of them, the accuracy was increased from 0.873 to 0.883. 
